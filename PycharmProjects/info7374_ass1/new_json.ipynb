{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "batch_size = 0\n",
    "num_classes = 0\n",
    "epochs = 0\n",
    "data_augmentation = True\n",
    "num_predictions = 0\n",
    "activation = \"\"\n",
    "metrics = \"\"\n",
    "\n",
    "time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))\n",
    "os.chdir(r'C:\\Users\\zaclge\\PycharmProjects\\info7374_ass1')\n",
    "save_dir = r'C:\\Users\\zaclge\\PycharmProjects\\info7374_ass1\\saved_models'\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "print(os.getcwd())\n",
    "def import_json(str_input):\n",
    "    \n",
    "    with open(r'C:\\Users\\zaclge\\PycharmProjects\\info7374_ass1\\configuration','r') as load_json:\n",
    "        load_dict = json.load(load_json)\n",
    "        print(load_dict)\n",
    "        global batch_size, num_classes, epochs, data_augmentation, num_predictions, activation, metrics\n",
    "        batch_size = load_dict[str_input][\"batch_size\"]\n",
    "        num_predictions = load_dict[str_input][\"num_predictions\"]\n",
    "        num_classes = load_dict[str_input][\"num_classes\"]\n",
    "        epochs = load_dict[str_input][\"epochs\"]\n",
    "        data_augmentation = load_dict[str_input][\"data_augmentation\"]\n",
    "        activation = load_dict[str_input][\"activation\"]\n",
    "        metrics = load_dict[str_input][\"metric\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress(paras):\n",
    "    # The data, shuffled and split between train and test sets:\n",
    "    import_json(paras)\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    # Convert class vectors to binary class matrices.\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "    \n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=[metrics])\n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        model.fit(x_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x_test, y_test),\n",
    "                  shuffle=True)\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "    \n",
    "        # Compute quantities required for feature-wise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        datagen.fit(x_train)\n",
    "    \n",
    "        # Fit the model on the batches generated by datagen.flow().\n",
    "        model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                         batch_size=batch_size),\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test, y_test),\n",
    "                            workers=4)\n",
    "    \n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    #model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "    \n",
    "    # Score trained model.\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': {'batch_size': 128, 'num_classes': 10, 'epochs': 100, 'data_augmentation': True, 'num_predictions': 20, 'activation': 'relu', 'metric': 'accuracy'}, 'two': {'batch_size': 128, 'num_classes': 10, 'epochs': 100, 'data_augmentation': True, 'num_predictions': 20, 'activation': 'relu', 'metric': 'top_k_categorical_accuracy'}, 'three': {'batch_size': 128, 'num_classes': 10, 'epochs': 100, 'data_augmentation': True, 'num_predictions': 20, 'activation': 'sigmoid', 'metric': 'accuracy'}}\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "391/391 [==============================] - 127s 325ms/step - loss: 1.9975 - acc: 0.2643 - val_loss: 1.7270 - val_acc: 0.3870\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 126s 321ms/step - loss: 1.7438 - acc: 0.3626 - val_loss: 1.5788 - val_acc: 0.4284\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 126s 322ms/step - loss: 1.6432 - acc: 0.4004 - val_loss: 1.5579 - val_acc: 0.4379\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 126s 321ms/step - loss: 1.5698 - acc: 0.4275 - val_loss: 1.4528 - val_acc: 0.4762\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 126s 323ms/step - loss: 1.5147 - acc: 0.4488 - val_loss: 1.3715 - val_acc: 0.5091\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 129s 330ms/step - loss: 1.4682 - acc: 0.4693 - val_loss: 1.3539 - val_acc: 0.5159\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 129s 330ms/step - loss: 1.4275 - acc: 0.4828 - val_loss: 1.3165 - val_acc: 0.5288\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 126s 322ms/step - loss: 1.3859 - acc: 0.4992 - val_loss: 1.2425 - val_acc: 0.5568\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 127s 326ms/step - loss: 1.3497 - acc: 0.5158 - val_loss: 1.1697 - val_acc: 0.5846\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 126s 322ms/step - loss: 1.3240 - acc: 0.5263 - val_loss: 1.2429 - val_acc: 0.5508\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 127s 324ms/step - loss: 1.2927 - acc: 0.5404 - val_loss: 1.1926 - val_acc: 0.5779\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 126s 322ms/step - loss: 1.2667 - acc: 0.5458 - val_loss: 1.1351 - val_acc: 0.5951\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 126s 321ms/step - loss: 1.2448 - acc: 0.5571 - val_loss: 1.1593 - val_acc: 0.5897\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 127s 324ms/step - loss: 1.2148 - acc: 0.5658 - val_loss: 1.0752 - val_acc: 0.6224\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 125s 321ms/step - loss: 1.1983 - acc: 0.5731 - val_loss: 1.0860 - val_acc: 0.6190\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 125s 320ms/step - loss: 1.1720 - acc: 0.5838 - val_loss: 1.0856 - val_acc: 0.6112\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 125s 320ms/step - loss: 1.1598 - acc: 0.5905 - val_loss: 1.0293 - val_acc: 0.6352\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 125s 320ms/step - loss: 1.1448 - acc: 0.5950 - val_loss: 0.9738 - val_acc: 0.6574\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 125s 320ms/step - loss: 1.1211 - acc: 0.6032 - val_loss: 0.9786 - val_acc: 0.6547\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 125s 320ms/step - loss: 1.1046 - acc: 0.6059 - val_loss: 1.0080 - val_acc: 0.6427\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 125s 320ms/step - loss: 1.0884 - acc: 0.6160 - val_loss: 0.9953 - val_acc: 0.6497\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 125s 320ms/step - loss: 1.0794 - acc: 0.6191 - val_loss: 0.9809 - val_acc: 0.6569\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 125s 320ms/step - loss: 1.0612 - acc: 0.6245 - val_loss: 0.9585 - val_acc: 0.6602\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - 125s 320ms/step - loss: 1.0572 - acc: 0.6279 - val_loss: 0.9742 - val_acc: 0.6598\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - 125s 321ms/step - loss: 1.0366 - acc: 0.6354 - val_loss: 0.9398 - val_acc: 0.6783\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - 125s 320ms/step - loss: 1.0275 - acc: 0.6385 - val_loss: 0.9866 - val_acc: 0.6567\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - 129s 329ms/step - loss: 1.0163 - acc: 0.6409 - val_loss: 0.9274 - val_acc: 0.6729\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - 144s 368ms/step - loss: 1.0053 - acc: 0.6460 - val_loss: 0.8894 - val_acc: 0.6869\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - 153s 391ms/step - loss: 0.9965 - acc: 0.6515 - val_loss: 0.9060 - val_acc: 0.6847\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - 156s 400ms/step - loss: 0.9830 - acc: 0.6539 - val_loss: 0.8928 - val_acc: 0.6840\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - 158s 404ms/step - loss: 0.9764 - acc: 0.6556 - val_loss: 0.8662 - val_acc: 0.6959\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - 155s 397ms/step - loss: 0.9662 - acc: 0.6606 - val_loss: 0.8281 - val_acc: 0.7061\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - 155s 397ms/step - loss: 0.9560 - acc: 0.6651 - val_loss: 0.8462 - val_acc: 0.7027\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - 156s 398ms/step - loss: 0.9428 - acc: 0.6691 - val_loss: 0.8373 - val_acc: 0.7066\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - 159s 406ms/step - loss: 0.9386 - acc: 0.6716 - val_loss: 0.8390 - val_acc: 0.7082\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - 156s 400ms/step - loss: 0.9285 - acc: 0.6742 - val_loss: 0.8674 - val_acc: 0.6994\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - 160s 408ms/step - loss: 0.9231 - acc: 0.6761 - val_loss: 0.8409 - val_acc: 0.7038\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - 153s 391ms/step - loss: 0.9176 - acc: 0.6787 - val_loss: 0.8067 - val_acc: 0.7177\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - 156s 398ms/step - loss: 0.9100 - acc: 0.6807 - val_loss: 0.8260 - val_acc: 0.7154\n",
      "Epoch 40/100\n",
      "391/391 [==============================] - 158s 405ms/step - loss: 0.9009 - acc: 0.6831 - val_loss: 0.8172 - val_acc: 0.7144\n",
      "Epoch 41/100\n",
      "391/391 [==============================] - 152s 389ms/step - loss: 0.8947 - acc: 0.6869 - val_loss: 0.8082 - val_acc: 0.7154\n",
      "Epoch 42/100\n",
      "391/391 [==============================] - 155s 395ms/step - loss: 0.8791 - acc: 0.6936 - val_loss: 0.7777 - val_acc: 0.7269\n",
      "Epoch 43/100\n",
      "391/391 [==============================] - 155s 397ms/step - loss: 0.8752 - acc: 0.6945 - val_loss: 0.7907 - val_acc: 0.7233\n",
      "Epoch 44/100\n",
      "391/391 [==============================] - 157s 400ms/step - loss: 0.8711 - acc: 0.6947 - val_loss: 0.7807 - val_acc: 0.7221\n",
      "Epoch 45/100\n",
      "391/391 [==============================] - 166s 424ms/step - loss: 0.8640 - acc: 0.6983 - val_loss: 0.7656 - val_acc: 0.7363\n",
      "Epoch 46/100\n",
      "391/391 [==============================] - 174s 445ms/step - loss: 0.8552 - acc: 0.7014 - val_loss: 0.7596 - val_acc: 0.7358\n",
      "Epoch 47/100\n",
      "391/391 [==============================] - 163s 417ms/step - loss: 0.8517 - acc: 0.7008 - val_loss: 0.7558 - val_acc: 0.7365\n",
      "Epoch 48/100\n",
      "391/391 [==============================] - 156s 399ms/step - loss: 0.8464 - acc: 0.7039 - val_loss: 0.7449 - val_acc: 0.7423\n",
      "Epoch 49/100\n",
      "391/391 [==============================] - 151s 387ms/step - loss: 0.8401 - acc: 0.7055 - val_loss: 0.7237 - val_acc: 0.7475\n",
      "Epoch 50/100\n",
      "391/391 [==============================] - 158s 405ms/step - loss: 0.8281 - acc: 0.7099 - val_loss: 0.7519 - val_acc: 0.7389\n",
      "Epoch 51/100\n",
      "391/391 [==============================] - 155s 396ms/step - loss: 0.8305 - acc: 0.7113 - val_loss: 0.8019 - val_acc: 0.7204\n",
      "Epoch 52/100\n",
      "391/391 [==============================] - 157s 402ms/step - loss: 0.8276 - acc: 0.7131 - val_loss: 0.7653 - val_acc: 0.7302\n",
      "Epoch 53/100\n",
      "391/391 [==============================] - 161s 413ms/step - loss: 0.8193 - acc: 0.7112 - val_loss: 0.6965 - val_acc: 0.7574\n",
      "Epoch 54/100\n",
      "391/391 [==============================] - 157s 401ms/step - loss: 0.8128 - acc: 0.7155 - val_loss: 0.7699 - val_acc: 0.7332\n",
      "Epoch 55/100\n",
      "391/391 [==============================] - 157s 401ms/step - loss: 0.8090 - acc: 0.7189 - val_loss: 0.7379 - val_acc: 0.7480\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 158s 403ms/step - loss: 0.8063 - acc: 0.7176 - val_loss: 0.7167 - val_acc: 0.7495\n",
      "Epoch 57/100\n",
      "391/391 [==============================] - 165s 422ms/step - loss: 0.8078 - acc: 0.7196 - val_loss: 0.7076 - val_acc: 0.7542\n",
      "Epoch 58/100\n",
      "391/391 [==============================] - 150s 385ms/step - loss: 0.7972 - acc: 0.7215 - val_loss: 0.6916 - val_acc: 0.7590\n",
      "Epoch 59/100\n",
      "391/391 [==============================] - 154s 395ms/step - loss: 0.7927 - acc: 0.7229 - val_loss: 0.7116 - val_acc: 0.7589\n",
      "Epoch 60/100\n",
      "391/391 [==============================] - 171s 437ms/step - loss: 0.7836 - acc: 0.7254 - val_loss: 0.6638 - val_acc: 0.7723\n",
      "Epoch 61/100\n",
      "391/391 [==============================] - 165s 423ms/step - loss: 0.7834 - acc: 0.7266 - val_loss: 0.7020 - val_acc: 0.7572\n",
      "Epoch 62/100\n",
      "391/391 [==============================] - 168s 430ms/step - loss: 0.7861 - acc: 0.7287 - val_loss: 0.7004 - val_acc: 0.7582\n",
      "Epoch 63/100\n",
      "391/391 [==============================] - 162s 414ms/step - loss: 0.7806 - acc: 0.7283 - val_loss: 0.7001 - val_acc: 0.7599\n",
      "Epoch 64/100\n",
      "391/391 [==============================] - 157s 401ms/step - loss: 0.7724 - acc: 0.7296 - val_loss: 0.6777 - val_acc: 0.7656\n",
      "Epoch 65/100\n",
      "391/391 [==============================] - 170s 435ms/step - loss: 0.7703 - acc: 0.7333 - val_loss: 0.6678 - val_acc: 0.7685\n",
      "Epoch 66/100\n",
      "391/391 [==============================] - 169s 433ms/step - loss: 0.7719 - acc: 0.7328 - val_loss: 0.6928 - val_acc: 0.7608\n",
      "Epoch 67/100\n",
      "391/391 [==============================] - 150s 385ms/step - loss: 0.7660 - acc: 0.7340 - val_loss: 0.6667 - val_acc: 0.7724\n",
      "Epoch 68/100\n",
      "391/391 [==============================] - 151s 387ms/step - loss: 0.7603 - acc: 0.7363 - val_loss: 0.6825 - val_acc: 0.7657\n",
      "Epoch 69/100\n",
      "391/391 [==============================] - 150s 385ms/step - loss: 0.7598 - acc: 0.7370 - val_loss: 0.6734 - val_acc: 0.7669\n",
      "Epoch 70/100\n",
      "391/391 [==============================] - 166s 424ms/step - loss: 0.7529 - acc: 0.7395 - val_loss: 0.6597 - val_acc: 0.7735\n",
      "Epoch 71/100\n",
      "391/391 [==============================] - 148s 378ms/step - loss: 0.7487 - acc: 0.7412 - val_loss: 0.7005 - val_acc: 0.7600\n",
      "Epoch 72/100\n",
      "391/391 [==============================] - 169s 432ms/step - loss: 0.7521 - acc: 0.7393 - val_loss: 0.6557 - val_acc: 0.7764\n",
      "Epoch 73/100\n",
      "391/391 [==============================] - 165s 421ms/step - loss: 0.7483 - acc: 0.7411 - val_loss: 0.6787 - val_acc: 0.7648\n",
      "Epoch 74/100\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 0.7471 - acc: 0.7411 - val_loss: 0.6508 - val_acc: 0.7762\n",
      "Epoch 75/100\n",
      "391/391 [==============================] - 149s 380ms/step - loss: 0.7386 - acc: 0.7438 - val_loss: 0.6376 - val_acc: 0.7810\n",
      "Epoch 76/100\n",
      "391/391 [==============================] - 136s 349ms/step - loss: 0.7328 - acc: 0.7457 - val_loss: 0.6387 - val_acc: 0.7804\n",
      "Epoch 77/100\n",
      "391/391 [==============================] - 136s 349ms/step - loss: 0.7338 - acc: 0.7465 - val_loss: 0.6678 - val_acc: 0.7701\n",
      "Epoch 78/100\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 0.7334 - acc: 0.7455 - val_loss: 0.6137 - val_acc: 0.7890\n",
      "Epoch 79/100\n",
      "391/391 [==============================] - 136s 349ms/step - loss: 0.7305 - acc: 0.7477 - val_loss: 0.6425 - val_acc: 0.7800\n",
      "Epoch 80/100\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 0.7204 - acc: 0.7512 - val_loss: 0.6504 - val_acc: 0.7737\n",
      "Epoch 81/100\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 0.7315 - acc: 0.7456 - val_loss: 0.6307 - val_acc: 0.7858\n",
      "Epoch 82/100\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 0.7250 - acc: 0.7497 - val_loss: 0.6477 - val_acc: 0.7797\n",
      "Epoch 83/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.7236 - acc: 0.7496 - val_loss: 0.6714 - val_acc: 0.7755\n",
      "Epoch 84/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.7193 - acc: 0.7514 - val_loss: 0.6279 - val_acc: 0.7838\n",
      "Epoch 85/100\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 0.7182 - acc: 0.7533 - val_loss: 0.6455 - val_acc: 0.7816\n",
      "Epoch 86/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.7163 - acc: 0.7535 - val_loss: 0.6216 - val_acc: 0.7889\n",
      "Epoch 87/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.7136 - acc: 0.7543 - val_loss: 0.6263 - val_acc: 0.7846\n",
      "Epoch 88/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.7143 - acc: 0.7526 - val_loss: 0.6465 - val_acc: 0.7777\n",
      "Epoch 89/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.7087 - acc: 0.7565 - val_loss: 0.6446 - val_acc: 0.7804\n",
      "Epoch 90/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.7089 - acc: 0.7564 - val_loss: 0.6398 - val_acc: 0.7807\n",
      "Epoch 91/100\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 0.7060 - acc: 0.7580 - val_loss: 0.6708 - val_acc: 0.7705\n",
      "Epoch 92/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.7076 - acc: 0.7566 - val_loss: 0.6079 - val_acc: 0.7928\n",
      "Epoch 93/100\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 0.7067 - acc: 0.7581 - val_loss: 0.6060 - val_acc: 0.7973\n",
      "Epoch 94/100\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 0.7005 - acc: 0.7600 - val_loss: 0.6461 - val_acc: 0.7815\n",
      "Epoch 95/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.7015 - acc: 0.7606 - val_loss: 0.6060 - val_acc: 0.7943\n",
      "Epoch 96/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.7022 - acc: 0.7575 - val_loss: 0.6321 - val_acc: 0.7827\n",
      "Epoch 97/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.6987 - acc: 0.7591 - val_loss: 0.6204 - val_acc: 0.7883\n",
      "Epoch 98/100\n",
      "391/391 [==============================] - 136s 349ms/step - loss: 0.7004 - acc: 0.7592 - val_loss: 0.6216 - val_acc: 0.7923\n",
      "Epoch 99/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.6957 - acc: 0.7614 - val_loss: 0.6206 - val_acc: 0.7902\n",
      "Epoch 100/100\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.6911 - acc: 0.7637 - val_loss: 0.6536 - val_acc: 0.7781\n",
      "Saved trained model at C:\\Users\\zaclge\\PycharmProjects\\info7374_ass1\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 9s 871us/step\n",
      "Test loss: 0.6536137211799622\n",
      "Test accuracy: 0.7781\n"
     ]
    }
   ],
   "source": [
    "progress(\"one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progress(\"two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
